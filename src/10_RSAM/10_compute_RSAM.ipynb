{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc74389a-0ad7-40f0-a256-f7c3260939cc",
   "metadata": {},
   "source": [
    "# RSAM notebook\n",
    "\n",
    "In this tutorial, we will explore Real-time Seismic Amplitude Measurement (RSAM) data. We will compute, visualize, analyze, and save RSAM data using the RSAM class from the GISMO module.\n",
    "\n",
    "## 1. Background\n",
    "\n",
    "### 1.1 Motivation\n",
    "\n",
    "Imagine it is Spring 1985, and you are at the only Seismologist at the USGS Cascades Volcano Observatory (CVO). Tremor is appearing on the helical drum recorders, and has appeared before most of the explosive eruptions over the past 5 years. The authorities want to know if the tremor now is as strong as it was right before the catastrophic May 18, 1980 sector collapse. \n",
    "\n",
    "Volcano-seismic monitoring was simple, and largely consisted of:\n",
    "1. Counting the number of earthquakes each day on the drum records (\"daily counts\")\n",
    "2. Locating and mapping volcano-tectonic earthquakes, and estimating their magnitudes (\"catalog production/analysis\")\n",
    "3. During heightened times of unrest, manning an Operations Room 24-7 with analysts continuously watching the drums, and communicating with field crews by 2-way radio (\"real-time monitoring\")\n",
    "\n",
    "So all you have is the drum records (hundreds of large sheets of paper) and the catalog. You don't have any digital version of the continuous seismic data sitting on a hard drive, or on a CD. Why? \n",
    "- CD-ROM drives didn't appear until ~1990\n",
    "- hard drive storage was too expensive. Here is a quick calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24904be6-8a21-4121-8c04-fadd3655d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm to compute raw storage space needed for seismic data\n",
    "def storage_space(samplingRate=100, bitsPerSample=32, numComponentsPerStation=3, numStations=10):\n",
    "    BITS_PER_BYTE = 8\n",
    "    SECONDS_PER_DAY = 60 * 60 * 24\n",
    "    bytesPerGb = 1024**3\n",
    "    gbPerDayPerChannel = (samplingRate * (bitsPerSample/BITS_PER_BYTE) * SECONDS_PER_DAY) / bytesPerGb\n",
    "    gbPerDayNetwork = gbPerDayPerChannel * numComponentsPerStation * numStations\n",
    "    print(f\"Raw data requires {gbPerDayNetwork:.02f} GB of storage per day, and {gbPerDayNetwork * 365:.0f} GB per year\")\n",
    "\n",
    "    dollarsPerTB = {'1985':31400000, '2000':4070, '2023':14.3}\n",
    "    print(\"\\nStorage cost for 1 year of data, in different years:\")\n",
    "    for key in dollarsPerTB:  \n",
    "        print(f\"{key}: US${(gbPerDayNetwork * 365 * dollarsPerTB[key]/1024):,.0f}\")\n",
    "\n",
    "    print(\"Data from https://ourworldindata.org/grapher/historical-cost-of-computer-memory-and-storage\")\n",
    "\n",
    "\n",
    "storage_space(samplingRate=100, bitsPerSample=32, numComponentsPerStation=3, numStations=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489305ac-af8b-4606-8c19-acec9c5c049f",
   "metadata": {},
   "source": [
    "So back in 1985, hard drive storage for just one year of data from the Mount St. Helens seismic network would have cost ~US$10 Million!\n",
    "Given these costs, STA/LTA algorithms were used to capture anomalous signals - volcanic earthquakes - while the continuous data were generally discarded (or at best, recorded to tape).\n",
    "\n",
    "Anyway, so you don't have an easy way to compare tremor levels. But you sure as hell aren't going to be caught in this situation again! So what can you do? <em><font color='green'>You can store a massively downsampled version of the continuous seismic data instead!</font></em>\n",
    "\n",
    "This idea led to the Real-time Seismic Amplitude Measurement (RSAM) system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69adc2-c75b-47c9-bd29-e9013f82dc53",
   "metadata": {},
   "source": [
    "### 1.2 Original RSAM system\n",
    "\n",
    "The RSAM system was built around a 8-bit analog-to-digital-converter PC card: sofware was too slow in those days. Components of the original RSAM system were:\n",
    "\n",
    "<font color='blue'>\n",
    "<ol>\n",
    "<li>Real-time bar graphs: showing average seismic amplitudes over last 2.56 s, 1 minute, and 10 minutes</li>\n",
    "<li><b>1 minute and 10 minute mean signal amplitudes, logged to binary files. This is what most volcano-seismologists today think of as \"RSAM data\"!</b></li>\n",
    "<li>\"RSAM events\": created by a simple STA/LTA detector running on each channel (NSLC)</li>\n",
    "<li>Multi-station event (e.g. earthquake) and tremor alarm systems</li>\n",
    "<li>Trends in RSAM data and other datasets (e.g. earthquake counts, tiltmeter data, gas flux, deformation, etc.) could be visualized with another software package called \"BOB\"</li>\n",
    "</ol></font>\n",
    "\n",
    "<table border=1><tr><td><img width=100% src=\"images/EndoMurray1991fig7.png\" ></td><td>Fig 7 from Endo & Murray (1991). Top panel shows RSAM event rate at closest station to Pinatubo. Bottom 3 panels show RSAM data from stations at increasing distances. 30 days of data are show</td></table></tr></table>\n",
    "\n",
    "In the figure above, 30 days of RSAM data are shown for three seismic stations. Loading and plotting 30 days of raw seismic data takes a while, but 1-minute RSAM data downsamples the raw seismic data by a factor of 6,000 (assuming a 100 Hz sampling rate), so long RSAM timeseries (hours, days, weeks, months, etc.) can be quickly loaded and plotted.\n",
    "\n",
    "Reference:\n",
    "- Endo, E.T., Murray, T. Real-time Seismic Amplitude Measurement (RSAM): a volcano monitoring and prediction tool. Bull Volcanol 53, 533â€“545 (1991).__[https://doi.org/10.1007/BF00298154](pdf/RSAM_EndoMurray1991.pdf)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ee2ae6-354d-469f-b2f0-2d4a02d93550",
   "metadata": {},
   "source": [
    "### 1.3 Loading legacy RSAM data from binary files\n",
    "\n",
    "Next we will load 1 year of RSAM data for 8 stations recorded by the original RSAM system that was deployed in Montserrat. And time how quickly it loads. I wrote an RSAM Class (in the Object-Oriented sense) which is part of the FLOVO.py package in the lib/ folder, to help with this and other exercises in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f21b10-8a61-4401-81c7-299c5a861fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import obspy\n",
    "sys.path.append('../lib')\n",
    "import setup_paths\n",
    "paths = setup_paths.paths\n",
    "from SAM import RSAM\n",
    "\n",
    "stime = obspy.core.UTCDateTime(1997,1,1,0,0,0)\n",
    "etime = obspy.core.UTCDateTime(1997,12,31,23,59,59)\n",
    "files = glob.glob(os.path.join(paths['SAMBINARY_DIR'], f'M???{stime.year}.DAT'))\n",
    "stations = list(set([os.path.basename(file)[0:4] for file in files]))\n",
    "print(stations)\n",
    "#stations = ['MCPZ', 'MGAT', 'MGHZ', 'MLGT', 'MRYT', 'MSPT', 'MSSZ', 'MWHT']\n",
    "rsamObj = RSAM.readRSAMbinary(paths['SAMBINARY_DIR'], stations, stime, etime)\n",
    "\n",
    "rsamObj.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b07a9ab-e778-4fa5-85e5-1fd7ee2d89db",
   "metadata": {},
   "source": [
    "### 1.4 Converting legacy RSAM binary files to modern RSAM CSV/Pickle files\n",
    "Since we have already read the binary files into a (single) RSAM object, writing them to modern RSAM data format is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b509f74-40e6-4a8a-a57c-b4ca40b4dd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsamObj.write(paths['SAM_DIR'], ext='pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a25a212-9ce5-4508-979e-f0db02379f74",
   "metadata": {},
   "source": [
    "### 1.5 Exploring RSAM objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7f9fcb-0efb-4fe3-9a7c-f95fc058597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(rsamObj)\n",
    "\n",
    "print(rsamObj)\n",
    "\n",
    "print('RSAM dataframe for one Trace id (net.sta.loc.chan):')\n",
    "print(rsamObj.dataframes['MV.MLGT..EHZ'].head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4db07b0b-658f-441b-ab08-e764f51ef2e1",
   "metadata": {},
   "source": [
    "## 2. Modern approach to computing RSAM data\n",
    "\n",
    "We don't nned to use specialist hardware these days to compute \"RSAM data\", or the BOB program to visualize \"RSAM data\": we can use Python and ObsPy! \n",
    "\n",
    "In Python/ObsPy code, the equation above is just:\n",
    "\n",
    "```python\n",
    "r = np.mean(np.absolute(tr.data))\n",
    "```\n",
    "where tr is a 1-minute long ObsPy Trace object. This calculation is repeated for each 1-minute time window. It's also a good idea to ```detrend``` each Trace first, to eliminate any offset or drift.\n",
    "\n",
    "The loop over 1-minute time windows can be vectorized using ```tr.data.reshape()```, which makes it faster. And why limit ourselves to 1-minute time windows, other good choices might be 1-second, 10-seconds, (1-minute), 10-minutes, 1-hour.\n",
    "\n",
    "And since we want a modern RSAM system, why just calculate the mean of each time window? We can compute other statistics too, including:\n",
    "- max (better for highlighting earthquakes)\n",
    "- median (better for eliminating spikes and highlighting \"tremor\")\n",
    "- mean in different frequency bands, e.g.:\n",
    "  - 0.02 - 0.2 Hz for highlighting very-long-period (VLP) seismicity\n",
    "  - 0.5 - 4.0 Hz for highlighting long-period (LP) seismicity\n",
    "  - 4.0 - 18.0 Hz for highlighting volcano-tectonic (VT) seismicity\n",
    "- frequency ratio: $log_2 (VT / LP)$ after Buurman & West (2010) and Rodgers et al. (2015)\n",
    "\n",
    "And rather than store in binary files, we convenient use Pandas dataframes and save data to CSV or Pickle files.\n",
    "\n",
    "References:\n",
    "- Buurman, H., and West, M.E., 2010. Seismic precursors to volcanic explosions during the 2006 eruption of Augustine Volcano, in Power, J.A., Coombs, M.L., and Freymueller, J.T., eds., The 2006 eruption of Augustine Volcano, Alaska: U.S. Geological Survey Professional Paper 1769. https://pubs.usgs.gov/pp/1769/chapters/p1769_chapter02.pdf\n",
    "- Rodgers, M. et al. 2015. Stable and unstable phases of elevated seismic activity at the persistently restless Telica Volcano, Nicaragua, J. Volcanol. Geotherm. Res., 90. https://doi.org/10.1016/j.jvolgeores.2014.11.012 https://doi.org/10.1016/j.jvolgeores.2014.11.012\n",
    "\n",
    "## 2.1 Computing RSAM data for 1-week of digital seismic network data from Montserrat\n",
    "\n",
    "To illustrate how to compute RSAM data for longer sequences of data, here is a fully worked example for Montserrat from February 26 to March 4, 2001. We will load raw seismic data in 1-day chunks from a SeisComP Data Structure (SDS) archive, and then compute corresponding 1-minute RSAM data.\n",
    "\n",
    "First we import all the necessary modules and set some constants that help with loading & saving data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7459cfd2-b85a-4842-b882-45be4fc37c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import obspy\n",
    "from obspy.clients.filesystem.sds import Client as sdsclient\n",
    "sys.path.append('../lib')\n",
    "import setup_paths\n",
    "paths = setup_paths.paths\n",
    "from SAM import RSAM\n",
    "\n",
    "mySDSclient = sdsclient(paths['SDS_DIR'])\n",
    "startTime = obspy.core.UTCDateTime(2001,2,26,0,0,0)\n",
    "endTime = obspy.core.UTCDateTime(2001,3,4,0,0,0)\n",
    "secondsPerDay = 60 * 60 * 24\n",
    "numDays = (endTime-startTime)/secondsPerDay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf574f-b80a-4f32-8667-26b8ab4e316d",
   "metadata": {},
   "source": [
    "Second, we loop over each day, loading data from all stations in the stMV (and ObsPy.Stream) and then computing RSAM, and saving RSAM data to pickle files. This might take 1-2 minutes, depending on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b10f3d-4185-4e8e-821c-d807a1e2dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "daytime = startTime\n",
    "while daytime < endTime:\n",
    "    time1 = time.time()\n",
    "    print(f'Loading Stream data for {daytime}')\n",
    "    st = mySDSclient.get_waveforms(\"MV\", \"*\", \"*\", \"[SBEHCD]*\", daytime, daytime+secondsPerDay)\n",
    "    print(f'- got {len(st)} Trace ids') \n",
    "\n",
    "    print(f'Computing RSAM metrics for {daytime}, and saving to pickle files')\n",
    "    rsamMV24h = RSAM(stream=st, sampling_interval=60)\n",
    "    #rsamMV24h.write(paths['SAM_DIR'], ext='pickle')\n",
    "    \n",
    "    daytime += secondsPerDay\n",
    "\n",
    "    time2=time.time()\n",
    "    print(f\"- day took {time2-time1} seconds\")\n",
    "\n",
    "del mySDSclient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dc366f-8c1d-4c9d-bca6-1ed03624753c",
   "metadata": {},
   "source": [
    "Next, we will load the RSAM data we just computed and saved, and print the RSAM object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d969a7-66d4-4038-abdd-267979ff4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = obspy.core.UTCDateTime(2000,3,19,0,0,0)\n",
    "endTime = obspy.core.UTCDateTime(2000,3,22,0,0,0)\n",
    "rsamMV = RSAM.read(startTime, endTime, SAM_DIR=paths['SAM_DIR'])\n",
    "print(rsamMV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f9bfc-2695-400d-a5db-1b99ccefb8da",
   "metadata": {},
   "source": [
    "In addition to 'mean', 8 other RSAM metrics are available. These are 'min', 'max', 'median', 'std', 'VLP', 'LP', 'VT', and 'specratio'. And we also see the sampling interval is 60 s, and the date/time range of the RSAM object goes from 2001/02/26 00:00 to 2001/03/30 14:42.\n",
    "\n",
    "Finally, we will plot some of these metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cdfeec-7d08-4b87-bb2d-44185ca6870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsamMV_Z = rsamMV.select(component='Z')\n",
    "rsamMV_Z.plot(metrics=['median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15084e33-5896-433c-b6ed-bca6670ae85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SAM import VSEM\n",
    "vsemMV = VSEM.read(startTime, endTime, SAM_DIR=paths['SAM_DIR'])\n",
    "print(vsemMV)\n",
    "vsemMV_Z = vsemMV.select(component='Z')\n",
    "vsemMV_Z.plot(metrics=['energy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce72446b-411d-419a-895c-4d9db4c1a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop IDs that don't look good\n",
    "rsamMV_Z.drop('MV.MBBY..BHZ')\n",
    "rsamMV_Z.drop('MV.MBMV..EHZ')\n",
    "\n",
    "# downsample from 1-minute to 10-minutes\n",
    "rsamMV_Zhourly = rsamMV_Z.downsample(new_sampling_interval=600)\n",
    "\n",
    "# plot mean & median, then LP and VT\n",
    "rsamMV_Z.plot(metrics=['mean', 'median'], kind='line')\n",
    "rsamMV_Zhourly.plot(metrics=['LP', 'VT'], kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d8a965-fadf-4624-aeea-f72a315f104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsamMV.plot(metrics=['specratio'], equal_scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c014dcf-748f-47b0-867d-16cbb452045e",
   "metadata": {},
   "source": [
    "# Exercise 1: make this exercise 2? put Montserrat example here?\n",
    "For this exercise, we are going to plot RSAM data recorded by the original RSAM system installed at Montserrat Volcano Observatory (MVO) that was installed in July 1995 by VDAP scientists. Specifically, we will look at data from summer 1996, which was my first time at MVO.\n",
    "\n",
    "## 1.1 First we do some imports, and set some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffde87e6-462e-4e37-a0c9-5cde331686a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../lib')\n",
    "import FLOVO\n",
    "import obspy\n",
    "RSAM_DIR = '../../data/continuous/RSAM'\n",
    "RSAM_BINARY_DIR = os.path.join(RSAM_DIR, 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b691b8-18dd-4b1e-a33b-fdf54bdd49e6",
   "metadata": {},
   "source": [
    "## 1.2. Convert RSAM binary files\n",
    "The original RSAM system recorded 1-minute RSAM data in binary files with a 4-byte float for each sample, for every minute of the year. \n",
    "Let us convert these files so we never have to bother with these binary files again. So we will read the whole year of 1996."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdad61e7-b808-40cb-b65c-a71182c04696",
   "metadata": {},
   "outputs": [],
   "source": [
    "stime = obspy.core.UTCDateTime(1996,1,1,0,0,0)\n",
    "etime = obspy.core.UTCDateTime(1996,12,31,23,59,59)\n",
    "stations = ['MCPZ', 'MGAT', 'MGHZ', 'MLGT', 'MRYT', 'MSPT', 'MSSZ', 'MWHT']\n",
    "rsamObj = FLOVO.readRSAMbinary(RSAM_BINARY_DIR, stations, stime, etime)\n",
    "#rsamObj.write(RSAM_DIR, ext='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dfd203-04ae-4b2d-ba55-62002e46e21b",
   "metadata": {},
   "source": [
    "The MVO network in 1996 didn't use modern SEED naming convention (network.station.location.channel or 'NSLC'), so here a network of 'MV' had been added, and the channel name has been set to 'EHZ' where 'E' indicates the original data were recorded at 100 Hz sampling rate, 'H' indicates seismic data, and 'Z' indicates a vertical component seismic sensor. \n",
    "\n",
    "'60s' has also been appended to the filename to indicate the sampling interval for the RSAM data. \n",
    "\n",
    "In memory, data for each station is stored in a pandas DataFrame, and on disk as a CSV file, which allows for easy browsing. \n",
    "\n",
    "To illustrate, next we will:\n",
    "\n",
    "- print the RSAM object\n",
    "- print the pandas DataFrame corresponding to id='MV.MLGT..EHZ'\n",
    "- inspect the corresponding CSV file\n",
    "- plot() the RSAM object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a434c7-431c-415e-b16a-0e1aa7d5b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RSAM object:')\n",
    "print(rsamObj)systems\n",
    "\n",
    "print('\\nDataFrame:')\n",
    "print(rsamObj.dataframes['MV.MLGT..EHZ'].head())\n",
    "\n",
    "print('\\nCSV file:')\n",
    "os.system('head ../../data/continuous/RSAM/RSAM_MV.MLGT..EHZ_1996_60s.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66938922-982a-4a42-9c2e-84182037e0ed",
   "metadata": {},
   "source": [
    "Each of these provides similar information. There are just two columns, which are 'time', and 'mean'. \n",
    "- 'time' is in Unix epoch seconds (since 1970-01-01 00:00:00)\n",
    "- 'mean' just holds the mean seismic amplitude within that 60-s time window (Sampling Interval=60.0s)\n",
    "\n",
    "# 1.3 Reading a subset of the data we just saved, and plotting an RSAM object\n",
    "Next we will re-read (from disk) the RSAM data from 1996-02-15 to 1996-10-12, and then plot the data. By default, the plot() method will convert RSAM dataframes into an ObsPy Stream object, so it can be plotted in a familiar way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07954ce8-d879-4131-9fd3-38ded80defc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "startt = obspy.core.UTCDateTime(1996,2,15)\n",
    "endt = obspy.core.UTCDateTime(1996,10,12)\n",
    "rsamObj = FLOVO.readRSAMfile(startt, endt, trace_ids=['MV.MLGT..EHZ', 'MV.MGAT..EHZ', 'MV.MRYT..EHZ', 'MV.MGHZ..EHZ'], RSAM_TOP=RSAM_DIR, ext='csv')\n",
    "rsamObj.plot()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4781f6-e8a7-4c2b-8d54-a2dc2e25a325",
   "metadata": {},
   "source": [
    "These RSAM plots above show the following general features:\n",
    "1. Low seismicity in February and March.\n",
    "2. An increase in seismicity around April 1st persists throughout to June. This period included the first pyroclastic density current (PDC) that reached the ocean on May 12, 1996.\n",
    "3. A more significant increase in activity about 2/3rds of the way through July 1996. This was a time period in which the seismicity and the lava dome extrusion rate significantly increased, leading to numerous PDCs that reached the ocean, and even travelled for some distance upon the water. The increase is particularly noticeable on MV.MLGT..EHZ (3rd trace) as this was close to the Tar River Valley, where most PDCs were directed.\n",
    "4. A sharp drop in seismicity from September 18, 1996, onwards.\n",
    "\n",
    "These features may be more obvious if we smooth the data, which we can do with the RSAMmetrics.smooth() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab0655-da4a-4cea-86f9-5c68cfb16b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rsamObj:\\n',rsamObj)\n",
    "rsamObjHourly = rsamObj.copy()\n",
    "rsamObjHourly.smooth(binsize=3600) \n",
    "print('rsamObjHourly:\\n',rsamObjHourly)\n",
    "rsamObjHourly.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9d3c8-0512-4af9-aab1-64bf8bf4970f",
   "metadata": {},
   "source": [
    "## 1.4 Trim, and Plot 1-minute RSAM data in summer 1996\n",
    "\n",
    "Let us zoom in on the hourly data we smoothed earlier. We can do this with the trim() method, followed by plot().\n",
    "trim() is done inplace, i.e. the original RSAM object is replaced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf386cc7-7f17-4b86-9d78-c41eb5a38aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "startt = obspy.core.UTCDateTime(1996,7,15)\n",
    "endt = obspy.core.UTCDateTime(1996,9,1)\n",
    "rsamObjHourly.trim(starttime=startt, endtime=endt)\n",
    "rsamObjHourly.plot(kind='stream', equal_scale=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fc7aeb-051c-4817-8298-e8f613762fab",
   "metadata": {},
   "source": [
    "There are various periods here where there seem to by cycles in RSAM. Let us look at early August period in more detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05921e9-a2fd-48a6-9aac-6f200de83020",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsamObjMidJuly = rsamObj.copy()\n",
    "rsamObjMidJuly.trim(starttime=obspy.core.UTCDateTime(1996,8,1), endtime=obspy.core.UTCDateTime(1996,8,8))\n",
    "rsamObjMidJuly.plot(kind='stream', equal_scale=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da22cb9-fde4-4056-a5c8-0b22ee2ba289",
   "metadata": {},
   "source": [
    "These are remarkable cycles in RSAM. They appear to be about 4-6 hours apart. This is a phenomenon called \"banded tremor\". During these tremor bands, visual observations indicated that the lava dome was extruding at particularly high rates (up to 20m^3 was one estimate I heard), and at the peak of each cycle there was often ash venting. I proposed that the tremor bands were indicated of pressure cycles within the conduit - but caused by what? \n",
    "One suggestion is that the magma rises up the conduit in a stick-slip fashion. Basically, it gets stuck for a while, as the pressure builds below, and then shear fractures, allowing magma to suddenly extrude very quickly. \n",
    "\n",
    "Can we use some ObsPy STA/LTA detection tools to detect these tremor bands, in the same way we normally detect much shorter transient events, but just with longer STA/LTA settings? Let us try first on a single NSLC. This is based on examples at https://docs.obspy.org/tutorial/code_snippets/trigger_tutorial.html, except we use longer STA and LTA time windows (15 and 100 minutes respectively), and we add a despiking step which attempts to remove transient events lasting a minute or less from the data before running the STA/LTA:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a5863d-89cb-429d-a95e-e036dcb6f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.signal.trigger import plot_trigger, classic_sta_lta, recursive_sta_lta\n",
    "st = rsamObjMidJuly._to_stream()\n",
    "st.trim(obspy.core.UTCDateTime(1996,8,3), obspy.core.UTCDateTime(1996,8,5))\n",
    "\n",
    "def despike(st, thresh=1.5):\n",
    "    for tr in st:\n",
    "        x = tr.data\n",
    "        for i in range(len(x)-2):\n",
    "            if x[i+1]>x[i]*thresh and x[i+1]>x[i+2]*thresh:\n",
    "                x[i+1] = (x[i] + x[i+2])/2\n",
    "        tr.data = x\n",
    "despike(st)\n",
    "trace = st[2].copy()\n",
    "trace.plot();\n",
    "\n",
    "sta_minutes = 15\n",
    "lta_minutes = 100\n",
    "threshON = 1.0\n",
    "threshOFF = 0.3\n",
    "\n",
    "cft = recursive_sta_lta(trace.data, sta_minutes, lta_minutes)\n",
    "\n",
    "plot_trigger(trace, cft, threshON, threshOFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c25c90a-5185-4386-8a03-05b9d5f04876",
   "metadata": {},
   "source": [
    "That seems to work quite well. Now let us try an event detector that uses several NSLC at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426f231-2701-4d1f-83a3-eed89c8a35e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.signal.trigger import coincidence_trigger\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "threshStations = 3\n",
    "\n",
    "trig = coincidence_trigger(\"recstalta\", threshON, threshOFF, st, threshStations, sta=sta_minutes*60, lta=lta_minutes*60, max_trigger_length=2*lta_minutes*60, delete_long_trigger=True)\n",
    "\n",
    "#pprint(trig)\n",
    "\n",
    "lendata = len(st[0].data)\n",
    "trdata = np.zeros( (lendata, ) )\n",
    "detectionTrace = obspy.Trace( data = trdata ) \n",
    "detectionTrace.id = 'XX.DETEC..TED'\n",
    "detectionTrace.stats.starttime = st[0].stats.starttime\n",
    "detectionTrace.stats.sampling_rate = st[0].stats.sampling_rate\n",
    "t = detectionTrace.times('utcdatetime')\n",
    "for thistrig in trig:\n",
    "    t0 = thistrig['time']\n",
    "    t1 = (thistrig['time'] + thistrig['duration'])\n",
    "    indices = np.where((t >= t0) & (t <= t1))\n",
    "    #print(t0, t1, indices)\n",
    "    detectionTrace.data[indices] = 1 #thistrig['duration']\n",
    "\n",
    "st3 = st.copy()\n",
    "st3.append(detectionTrace)\n",
    "st3.plot(equal_scale=False);\n",
    "\n",
    "detection_ON_times = [thistrig['time'].timestamp for thistrig in trig]\n",
    "detection_intervals_minutes = np.diff(np.array(detection_ON_times))/60\n",
    "for i,d in enumerate(detection_intervals_minutes):\n",
    "    print(f\"detection ON time for band {i}: {trig[i]['time']}, duration: {trig[i]['duration']/60} mins\")\n",
    "    print(f\"- interval (mins): {detection_intervals_minutes[i]}\")\n",
    "print(f\"detection ON time for band {i+1}: {trig[i+1]['time']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9499ef3a-527e-4675-b00c-ca6428c7bf54",
   "metadata": {},
   "source": [
    "The bottom trace here corresponds to the detected events, and you can see they line up pretty well with the tremor bands, except the first one was missed.\n",
    "\n",
    "This is similar to the banded tremor alarm system I wrote at MVO in 2000. And using this approach we can forecast the timing of the next tremor band. As it was the MVO Seismologist's job to manage the Operations Room, which included continuous seismic monitoring and two-way radio communications with MVO field crews, it was useful to predict tremor bands, as these were periods of heightened activity when field crews should not be on the flanks of the volcano.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc607da-f52d-4a22-a4ef-46c0e0021869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further isolate bands by stacking and applying detection mask\n",
    "stackedTrace = st3[0].copy()\n",
    "stackedTrace.id = 'MV.STACK..EHZ'\n",
    "for tr in st3[1:-1]:\n",
    "    stackedTrace.data += tr.data\n",
    "stackedTrace.plot();\n",
    "\n",
    "maskedTrace = stackedTrace.copy()\n",
    "#maskedTrace.stats.station = 'MASK'\n",
    "#maskedTrace.data *= st3[-1].data\n",
    "\n",
    "# find peak value and peak time during each band\n",
    "import pandas as pd\n",
    "lod = []\n",
    "for thistrig in trig:\n",
    "    bandstarttime = thistrig['time']\n",
    "    bandendtime = thistrig['time'] + thistrig['duration']\n",
    "    bandTrace = maskedTrace.copy().trim(starttime=bandstarttime, endtime=bandendtime)\n",
    "    bandpeaktime = bandstarttime + bandTrace.data.argmax() * tr.stats.delta\n",
    "    band = {'starttime':bandstarttime, 'waxtime':bandpeaktime-bandstarttime, \\\n",
    "            'peaktime':bandpeaktime, 'wanetime':bandendtime-bandpeaktime, 'endtime':bandendtime, 'duration':thistrig['duration']}\n",
    "    lod.append(band)\n",
    "\n",
    "\n",
    "bandDf = pd.DataFrame(lod)\n",
    "print(bandDf)\n",
    "\n",
    "predicted = []\n",
    "for col in ['starttime', 'peaktime', 'endtime']:\n",
    "    interval = (bandDf.iloc[-1][col] - bandDf.iloc[0][col]) / (len(bandDf)-1) \n",
    "    predicted.append(bandDf.iloc[-1][col] + interval)\n",
    "print('\\nNext band prediction:')\n",
    "print(' - start: ',predicted[0])\n",
    "print(' - peak:  ',predicted[1])\n",
    "print(' - end:   ',predicted[2])\n",
    "\n",
    "st5 = rsamObjMidJuly.copy()\n",
    "st5.trim(starttime = obspy.core.UTCDateTime(1996,8,5,0,0,0), endtime = obspy.core.UTCDateTime(1996,8,5,3,0,0) )\n",
    "st5.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc51fe28-0725-4463-a5ad-de1303758a78",
   "metadata": {},
   "source": [
    "# Exercise 2: \n",
    "For this exercise, we are going to use continuous seismic data from Montserrat in late February/early March, 2001, and compute our own RSAM data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e799c418-d508-4dd1-a0a5-a8500bd698a3",
   "metadata": {},
   "source": [
    "# 2.2 Redoubt Volcano, Alaska, 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5307b9d3-4bc6-4d3c-b3c8-be06eb698c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compute for Redoubt\n",
    "stime = obspy.core.UTCDateTime(2009,3,22,22,0,0)\n",
    "etime = obspy.core.UTCDateTime(2009,3,23,8,0,0)\n",
    "from obspy.clients.filesystem.sds import Client as SDSCLIENT\n",
    "myclient = SDSCLIENT('../../data/continuous/SDS')\n",
    "stRD = myclient.get_waveforms(\"AV\", \"*\", \"*\", \"[HDESB]*\", stime, etime)\n",
    "#print(stRD.__str__(extended=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ae0deb-3422-4143-b451-1e8064e53797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to merge traces by rounding off the sampling rate\n",
    "stRD.sort(['starttime', 'endtime', 'npts', 'network', 'station', 'location', 'channel'])\n",
    "stRD2 = stRD.copy()\n",
    "try:\n",
    "    stRD2.merge(method=0, fill_value='interpolate')\n",
    "except:\n",
    "    stRD2 = obspy.core.Stream()\n",
    "    for tr in stRD:\n",
    "        try:\n",
    "            tr.stats.sampling_rate=round(tr.stats.sampling_rate)\n",
    "            tr.data[abs(tr.data)>10000]=0\n",
    "            stRD2.append(tr)\n",
    "            stRD2.merge(method=0, fill_value='interpolate')\n",
    "            #print(tr)\n",
    "        except:\n",
    "            print(f\"Cannot merge {tr}\")\n",
    "stRD2.plot(starttime=stime+4.8*3600, endtime=stime+5*3600, equal_scale=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1430ef-a6d4-4315-98db-0e035a7a4edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_ids = ['AV.RDN..EHZ', 'AV.REF..EHZ', 'AV.REF..EHN', 'AV.REF..EHE', 'AV.RSO..EHZ', 'AV.RDWB..BHZ', 'AV.RDWB..BHN', 'AV.RDWB..BHE']\n",
    "stRD3 = obspy.core.Stream()\n",
    "for id in good_ids:\n",
    "    stRD3.append(stRD2.select(id=id)[0])\n",
    "importlib.reload(FLOVO)\n",
    "r2 = FLOVO.RSAMmetrics(stream=stRD3, sampling_interval=60, filter=True)\n",
    "r2.plot(kind='stream', metrics=['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebce2d0-0e0f-43c8-a186-8a2dc08e299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2.plot(kind='stream', metrics=['specratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1f04a-72fb-4900-9f18-ae748ff32fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stRD4 = stRD3.copy().trim(starttime=stime+4.8*3600, endtime=stime+5*3600)\n",
    "importlib.reload(FLOVO)\n",
    "r3 = FLOVO.RSAMmetrics(stream=stRD4, sampling_interval=3, filter=True)\n",
    "r3.plot(kind='stream');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6550ea1a-06ab-4709-89eb-990ff826f746",
   "metadata": {},
   "source": [
    "## 2.3 Soufriere Hills Volcano, Montserrat, 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3891f7e9-4541-456a-b81c-9840e124eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import obspy\n",
    "from obspy.clients.filesystem.sds import Client as sdsclient\n",
    "\n",
    "PROJECT_DIR = os.path.join('..','..')\n",
    "LIB_DIR = os.path.join(PROJECT_DIR,'lib')\n",
    "sys.path.append(LIB_DIR)\n",
    "import FLOVO\n",
    "\n",
    "CONTINUOUS_DATA_DIR = os.path.join(PROJECT_DIR, 'data', 'continuous')\n",
    "SDS_DIR = os.path.join(CONTINUOUS_DATA_DIR, 'SDS')\n",
    "RSAM_DIR = os.path.join(CONTINUOUS_DATA_DIR, 'RSAM')\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "mySDSclient = sdsclient(SDS_DIR)\n",
    "startTime = obspy.core.UTCDateTime(2001,2,25,0,0,0)\n",
    "endTime = obspy.core.UTCDateTime(2001,3,5,0,0,0)\n",
    "secondsPerDay = 60 * 60 * 24\n",
    "numDays = (endTime-startTime)/secondsPerDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31abbbb-3cd6-4c6a-96dd-6c07b17d06c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr2 = obspy.core.Trace(data=np.array(rsam_values))\n",
    "tr2.stats.starttime = stime\n",
    "tr2.stats.delta = time_window\n",
    "tr2.id = tr.id\n",
    "tr2.stats.channel = 'U' + tr.stats.channel[1:]\n",
    "\n",
    "st = obspy.core.Stream([tr, tr2])\n",
    "st.plot(equal_scale=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed65ba46-e224-4847-b9a6-97f6fbc11a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('median ',np.median(tr.data))\n",
    "print('max ',np.max(tr.data))\n",
    "print('std ',np.std(tr.data))\n",
    "print('99th percentile ',np.percentile(tr.data, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5796ab-d0b0-4652-a42c-cd4c3982732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = trMBWH.times() # seconds from start of seismogram\n",
    "udt = trMBWH.stats.starttime\n",
    "time_window = 60 # 60-s time window\n",
    "rsam_values = []\n",
    "rsam_times = []\n",
    "for startt in range(t[0], t[-1], time_window):\n",
    "    tr_segment = trMBWH.copy().trim(starttime=udt+startt, endtime=udt+startt+time_window)\n",
    "    m = np.mean(tr_segment.data)\n",
    "    rsam_times.append(u+startt+time_window)\n",
    "    rsam_values.append(m)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rsam_times.datetime, rsam_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a665a0c1-96a5-4049-b045-0f21fbb55db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "daytime = startTime\n",
    "while daytime < endTime:\n",
    "    print(f'Loading Stream data for {daytime}')\n",
    "    stMV = mySDSclient.get_waveforms(\"MV\", \"*\", \"*\", \"[SBEHCD]*\", daytime, daytime+secondsPerDay)\n",
    "\n",
    "    print(f'Computing RSAM metrics for {daytime}, and saving to CSV')\n",
    "    rsamMV24h = FLOVO.RSAM(stream=stMV, sampling_interval=60)\n",
    "    rsamMV24h.write(RSAM_DIR)\n",
    "    \n",
    "    daytime += secondsPerDay\n",
    "\n",
    "del mySDSclient\n",
    "\n",
    "print(f'Reading and Plotting {numDays} days of RSAM metrics')\n",
    "\n",
    "\n",
    "rsamMV = FLOVO.readRSAMfile(startTime, endTime, trace_ids=None, RSAM_TOP=RSAM_DIR)\n",
    "print(rsamMV)\n",
    "rsamMV.plot(kind='stream', metrics=['median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b862d06-6fcc-417a-a8c2-8a9b4ff655bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "testtuple = (2003, 2, 25)\n",
    "teststart = obspy.core.UTCDateTime(testtuple[0], testtuple[1], testtuple[2])\n",
    "print(teststart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd6e50-7033-4fdc-8916-5456555aa089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add frequency index to RSAMmetrics\n",
    "# add a 1/rsam plot for FFM\n",
    "# add a way to compute RSAM for overlapping time bins - easy to do if say select 1 minute first, and then apply say a 10-point moving point filter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
